#+title: TODO
#+author: swh team

* DONE swh implementation - poc 1
CLOSED: [2015-07-22 Wed 12:20]

- [X] Push on remote git repository
- [X] All git objects must be written in storage (at the moment only blobs)
- [X] Improve performance
- [X] Serialize blob's data and not blob's size.
- [X] Logging in python? How to see the log?
- [X] Replace sqlalchemy dao layer with psycopg2
- [X] Improve sgloader cli interface
- [X] Serialize sha256 as bytes
- [X] Update README.org
- [X] Switch dao layer (from sqlalchemy to psycopg2)
- [X] Serialize sha1 as bytes
- [X] Use sha1 instead of sha256 for file cache
- [X] Improve architecture
- [X] Use postgresql's bytea column for sha1
- [X] Improve git object dispatch (look up on repo object only if necessary)
- [X] Add functional test which adds new commits
- [X] Store git object on disk too
- [X] Make the compression for the file storage optional
- [X] Expose the flag to the swh-git-loader's configuration
- [X] Make the compression for the git object storage optional
- [X] Expose option flag for blob compression
- [X] Add computation folder with depth as parameter
- [X] Expose option flag for folder depth
- [X] Test coverage for at least primitives functions [2/2]
  - [X] swh.file
  - [X] swh.hash
- [X] Add git-sha1 function in swh.hash module
- [X] Separate the git repository parsing from the persistence (using backend api)
- [X] Enforce retrying disk writing policy
- [X] Use blob's git sha1 as key on disk
- [X] Enforce retrying policy on http client requests
- [X] Share http connection throughout the git repository parsing

* IN-PROGRESS swh implementation - poc 2

- [X] One content storage (2 were used, one for content, one for revision/directory)
- [X] Improve api backend to use the `real` schema
- [X] Adapt loader to speak the api backend the dummy way (~json)
- [X] Clean up some db-manager code + adapt makefile to use swh-sql
- [X] Improve protocol communication between loader and api backend (drop json) > pickle

- [ ] Roberto: do not use specific language serialization stack (pickle) -> the backend api could not be that private after all.
- [ ] Roberto: add md5 checksum on contents to make sure there were no integrity during transport
- [ ] Fix multiple FIXMEs (`M-x rgrep RET FIXME RET *.py RET /path/to/swh-git-loader RET`)
  - [ ] Use sha256 with sha1 for content filtering to backend
  - [ ] Find the right times (atime, ctime, mtime)
  - [ ] Determine if `swhmap` data structure is really useful or not -> could be a simple map right now...
  - [ ] ...

- [ ] Deal with sha1s collisions
- [ ] Ultimately, the swh-git-loader is not distributed right now. Determine if we need it to run with celery and then adapt code accordingly.

* Global enhancement

** Implementation details

- Pointers on raw data instead of in memory representation -> cf. read_raw() call in `swh/gitloader/git.py`...

- Maybe loader's repository memory model on disk and then open a file handler and provides this to the http client when ready
-> pickle (for the moment) knows how to deal with it
-> flask can deal with it (content-type header is 'application/octet-stream')

** Performance

Of course, we'd need to measure the actual performance first to determine if we could improve on it.

But some hints:
Use future computations. Can we send in // some data?
-> POST contents' signatures
-> POST directories' signatures
-> POST revisions' signatures

** Collision

We need to deal with potential collisions.

Backend check for collision and warn the loader if there is.

The loader deals with the collision message accordingly.

-> if everything is ok, when done, sends a message to queue to say it's ok to delete the repository
-> if collisions has been detected, at the end of it all, sends a message to another error queue to say `WARNING collision, do not destroy this repository`

* Discussion

** How to stream blob's data

Returned as raw data

** Structure log

This way they could serve for analysis by other mechanism

** Rules

**** Don't lose data

Multiple workers.
Same disks and db.

**** Transaction

Unit of transaction.
Reading if a commit exists, if not write on disk + on db.
If one disk fails, fail the transaction.

**** ?

**** Profiling
Look into the cursor implementation details.
